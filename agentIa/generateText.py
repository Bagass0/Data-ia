"""
Agent IA avec Function Calling et exÃ©cution multi-Ã©tapes
Utilise l'API Mistral AI pour gÃ©nÃ©rer du texte et exÃ©cuter des actions
"""

import json
import os
import subprocess
import requests
from typing import Optional, List, Dict
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Configuration
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
DEFAULT_MODEL = "mistral-tiny"
DEFAULT_TEMPERATURE = 0.7
API_TIMEOUT = 30
SCRIPT_TIMEOUT = 30


def generateText(prompt: str, model: str = DEFAULT_MODEL, temperature: float = DEFAULT_TEMPERATURE, max_tokens: Optional[int] = None) -> str:
    """
    GÃ©nÃ¨re du texte en utilisant l'API Mistral AI.
    
    Args:
        prompt: Le prompt Ã  envoyer Ã  l'API
        model: Le modÃ¨le Ã  utiliser
        temperature: ContrÃ´le la crÃ©ativitÃ© (0.0 Ã  1.0)
        max_tokens: Limite le nombre de tokens
    
    Returns:
        Le texte gÃ©nÃ©rÃ© par l'API
    
    Raises:
        Exception: Si l'API retourne une erreur
    """
    if not MISTRAL_API_KEY:
        raise ValueError("ClÃ© API Mistral manquante dans le fichier .env")
    
    url = "https://api.mistral.ai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {MISTRAL_API_KEY}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": model,
        "messages": [{"role": "user", "content": prompt}],
        "temperature": temperature
    }
    
    if max_tokens is not None:
        data["max_tokens"] = max_tokens
    
    try:
        response = requests.post(url, headers=headers, json=data, timeout=API_TIMEOUT)
        
        if response.status_code == 200:
            result = response.json()
            if "choices" in result and len(result["choices"]) > 0:
                return result["choices"][0]["message"]["content"]
            else:
                raise Exception("Structure de rÃ©ponse inattendue de l'API Mistral")
        else:
            error_msg = f"Erreur API Mistral: {response.status_code}"
            try:
                error_details = response.json()
                if "error" in error_details:
                    error_msg += f" - {error_details['error']}"
            except:
                error_msg += f" - {response.text}"
            raise Exception(error_msg)
    
    except requests.exceptions.RequestException as e:
        raise Exception(f"Erreur de connexion Ã  l'API Mistral: {str(e)}")


def writeFile(path: str, content: str) -> str:
    """
    Ã‰crit du contenu dans un fichier dans le dossier 'generated'.
    
    Args:
        path: Le chemin du fichier Ã  crÃ©er (sera placÃ© dans generated/)
        content: Le contenu Ã  Ã©crire
    
    Returns:
        Message de confirmation
    """
    try:
        # CrÃ©er le dossier generated s'il n'existe pas
        generated_dir = "generated"
        if not os.path.exists(generated_dir):
            os.makedirs(generated_dir)
        
        # Construire le chemin complet dans le dossier generated
        full_path = os.path.join(generated_dir, path)
        
        with open(full_path, 'w', encoding='utf-8') as f:
            f.write(content)
        return f"âœ… Fichier crÃ©Ã© avec succÃ¨s : {full_path}"
    except Exception as e:
        return f"âŒ Erreur lors de la crÃ©ation du fichier : {str(e)}"


def launchPythonFile(path: str) -> str:
    """
    Lance un fichier Python depuis le dossier 'generated'.
    
    Args:
        path: Le chemin du fichier Python Ã  exÃ©cuter
    
    Returns:
        Le rÃ©sultat de l'exÃ©cution
    """
    try:
        # Normaliser le chemin - enlever les "generated/" en double
        if path.startswith("generated/") or path.startswith("generated\\"):
            # Le chemin contient dÃ©jÃ  generated/, on l'utilise tel quel
            full_path = path.replace("\\", "/")  # Normaliser les sÃ©parateurs
        else:
            # Ajouter generated/ devant
            generated_dir = "generated"
            full_path = os.path.join(generated_dir, path).replace("\\", "/")
        
        # VÃ©rifier que le fichier existe
        if not os.path.exists(full_path):
            # Essayer aussi sans le prÃ©fixe generated/ au cas oÃ¹
            alt_path = path
            if os.path.exists(alt_path):
                full_path = alt_path
            else:
                return f"âŒ Fichier non trouvÃ© : {full_path} (aussi testÃ©: {alt_path})"
        
        result = subprocess.run(['python', full_path], capture_output=True, text=True, timeout=SCRIPT_TIMEOUT)
        if result.returncode == 0:
            return f"âœ… ExÃ©cution rÃ©ussie :\n{result.stdout}"
        else:
            return f"âŒ Erreur d'exÃ©cution :\n{result.stderr}"
    except subprocess.TimeoutExpired:
        return "â±ï¸ Timeout : Le script a pris trop de temps Ã  s'exÃ©cuter"
    except Exception as e:
        return f"âŒ Erreur lors du lancement : {str(e)}"


def readFile(path: str) -> str:
    """
    Lit le contenu d'un fichier depuis le dossier 'generated'.
    
    Args:
        path: Le chemin du fichier Ã  lire
    
    Returns:
        Le contenu du fichier ou un message d'erreur
    """
    try:
        # Normaliser le chemin - enlever les "generated/" en double
        if path.startswith("generated/") or path.startswith("generated\\"):
            # Le chemin contient dÃ©jÃ  generated/, on l'utilise tel quel
            full_path = path.replace("\\", "/")
        else:
            # Ajouter generated/ devant
            generated_dir = "generated"
            full_path = os.path.join(generated_dir, path).replace("\\", "/")
        
        # VÃ©rifier que le fichier existe
        if not os.path.exists(full_path):
            # Essayer aussi sans le prÃ©fixe generated/ au cas oÃ¹
            alt_path = path
            if os.path.exists(alt_path):
                full_path = alt_path
            else:
                return f"âŒ Fichier non trouvÃ© : {full_path}"
        
        with open(full_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return f"âœ… Contenu du fichier {full_path} :\n{content}"
    except Exception as e:
        return f"âŒ Erreur lors de la lecture du fichier : {str(e)}"


def stop() -> str:
    """
    Fonction pour arrÃªter l'agent.
    
    Returns:
        Message d'arrÃªt
    """
    return "ğŸ›‘ Agent arrÃªtÃ© - TÃ¢che terminÃ©e"


class Agent:
    """Agent IA avec capacitÃ© d'exÃ©cution multi-Ã©tapes"""
    
    def __init__(self):
        self.available_functions = {
            "writeFile": writeFile,
            "launchPythonFile": launchPythonFile,
            "readFile": readFile,
            "stop": stop
        }
        self.execution_history = []
    
    def get_function_calling_prompt(self, user_message: str, history: List[str] = None) -> str:
        """
        CrÃ©Ã© un prompt pour le function calling avec historique.
        
        Args:
            user_message: Le message de l'utilisateur
            history: Historique des actions prÃ©cÃ©dentes
        
        Returns:
            Le prompt formatÃ©
        """
        history_text = ""
        if history:
            history_text = f"\n\nHistorique des actions prÃ©cÃ©dentes :\n" + "\n".join(history)
        
        return f"""Tu es un assistant qui peut utiliser des fonctions pour accomplir des tÃ¢ches complexes.

Fonctions disponibles :

1. writeFile(path, content)
   - CrÃ©e un fichier avec le contenu spÃ©cifiÃ©
   - path: chemin du fichier (string)
   - content: contenu Ã  Ã©crire (string)

2. launchPythonFile(path)
   - Lance un fichier Python
   - path: chemin du fichier Python (string)

3. readFile(path)
   - Lit le contenu d'un fichier
   - path: chemin du fichier (string)

4. stop()
   - ArrÃªte l'agent quand la tÃ¢che est terminÃ©e
   - Aucun argument requis

Demande de l'utilisateur: "{user_message}"{history_text}

Si tu dois utiliser une fonction, rÃ©ponds UNIQUEMENT avec un JSON valide dans ce format :
{{
    "function_name": "nom_de_la_fonction",
    "arguments": {{
        "argument1": "valeur1",
        "argument2": "valeur2"
    }}
}}

Si la tÃ¢che est terminÃ©e, utilise la fonction "stop".
Si tu n'as pas besoin d'utiliser une fonction maintenant, rÃ©ponds normalement.

Ne rÃ©ponds QUE avec le JSON ou un texte normal, pas les deux."""

    def execute_function_call(self, function_name: str, arguments: Dict) -> str:
        """
        ExÃ©cute une fonction basÃ©e sur son nom et ses arguments.
        
        Args:
            function_name: Le nom de la fonction Ã  exÃ©cuter
            arguments: Les arguments Ã  passer Ã  la fonction
        
        Returns:
            Le rÃ©sultat de l'exÃ©cution
        """
        if function_name not in self.available_functions:
            return f"âŒ Fonction inconnue : {function_name}"
        
        try:
            function = self.available_functions[function_name]
            if arguments:
                result = function(**arguments)
            else:
                result = function()
            return result
        except Exception as e:
            return f"âŒ Erreur lors de l'exÃ©cution de {function_name} : {str(e)}"

    def process_llm_response(self, response: str, step: int) -> tuple:
        """
        Traite la rÃ©ponse du LLM et exÃ©cute les fonctions si nÃ©cessaire.
        
        Args:
            response: La rÃ©ponse du LLM
            step: NumÃ©ro de l'Ã©tape
        
        Returns:
            Tuple (rÃ©sultat, should_stop)
        """
        try:
            response_json = json.loads(response.strip())
            
            if "function_name" in response_json and "arguments" in response_json:
                function_name = response_json["function_name"]
                arguments = response_json.get("arguments", {})
                
                print(f"ğŸ”§ Ã‰tape {step}: ExÃ©cution de la fonction : {function_name}")
                print(f"ï¿½ Arguments : {arguments}")
                
                # ExÃ©cuter la fonction
                result = self.execute_function_call(function_name, arguments)
                
                # Ajouter Ã  l'historique
                history_entry = f"Ã‰tape {step}: {function_name}({arguments}) -> {result}"
                self.execution_history.append(history_entry)
                
                # VÃ©rifier si c'est la fonction stop
                should_stop = function_name == "stop"
                
                return result, should_stop
            else:
                return response, False
        except json.JSONDecodeError:
            # Si ce n'est pas du JSON, c'est une rÃ©ponse normale
            return response, False

    def run_agent(self, prompt: str, max_step: int = 10) -> List[str]:
        """
        Lance l'agent pour accomplir une tÃ¢che avec plusieurs Ã©tapes.
        
        Args:
            prompt: La demande initiale de l'utilisateur
            max_step: Nombre maximum d'Ã©tapes
        
        Returns:
            Liste des rÃ©sultats de chaque Ã©tape
        """
        print(f"ğŸš€ DÃ©marrage de l'agent pour : {prompt}")
        print(f"ğŸ“Š Maximum {max_step} Ã©tapes")
        print("=" * 60)
        
        results = []
        self.execution_history = []
        
        for step in range(1, max_step + 1):
            print(f"\nğŸ¯ Ã‰TAPE {step}/{max_step}")
            print("-" * 30)
            
            # CrÃ©er le prompt avec l'historique
            function_prompt = self.get_function_calling_prompt(prompt, self.execution_history)
            
            try:
                # GÃ©nÃ©rer la rÃ©ponse
                print("ğŸ¤” RÃ©flexion en cours...")
                response = generateText(function_prompt)
                
                # Traiter la rÃ©ponse
                result, should_stop = self.process_llm_response(response, step)
                results.append(result)
                
                print(f"ï¿½ RÃ©sultat : {result}")
                
                # VÃ©rifier si l'agent veut s'arrÃªter
                if should_stop:
                    print(f"\nğŸ Agent arrÃªtÃ© Ã  l'Ã©tape {step}")
                    break
                    
            except Exception as e:
                error_msg = f"âŒ Erreur Ã  l'Ã©tape {step}: {str(e)}"
                print(error_msg)
                results.append(error_msg)
                break
        
        print("\n" + "=" * 60)
        print("ğŸ“Š RÃ‰SUMÃ‰ DE L'EXÃ‰CUTION")
        print("=" * 60)
        
        for i, result in enumerate(results, 1):
            print(f"Ã‰tape {i}: {result}")
        
        return results


def chat_interface():
    """Interface de chat avec l'agent"""
    print("ğŸ¤– Agent IA Mistral avec Function Calling Multi-Ã‰tapes")
    print("ğŸ“‹ Fonctions disponibles : writeFile, launchPythonFile, stop")
    print("ğŸ”„ Commandes spÃ©ciales :")
    print("   - 'agent <demande>' : Lance l'agent multi-Ã©tapes")
    print("   - 'quit' : Quitter")
    print("=" * 60)
    
    agent = Agent()
    
    while True:
        try:
            user_input = input("\nğŸ’¬ Vous: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q', 'bye']:
                print("ğŸ‘‹ Au revoir!")
                break
            
            if not user_input:
                print("âŒ Veuillez taper quelque chose...")
                continue
            
            # VÃ©rifier si c'est une demande d'agent multi-Ã©tapes
            if user_input.lower().startswith('agent '):
                task = user_input[6:]  # Enlever 'agent '
                max_steps = 5  # Par dÃ©faut 5 Ã©tapes
                agent.run_agent(task, max_steps)
            else:
                # Utilisation normale du chat
                print("ğŸ¤” RÃ©flexion en cours...")
                response = generateText(user_input)
                print(f"ğŸ¤– Assistant: {response}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Au revoir!")
            break
        except Exception as e:
            print(f"âŒ Erreur: {e}")
            print("ğŸ”„ RÃ©essayez...")


if __name__ == "__main__":
    chat_interface()
